{
  "thr": 0.723724633148036,
  "thr_by_cat": {
    "public and personal health": 0.9231779998348605,
    "spam": 0.6425448819994927,
    "hate": 0.7680014323948615,
    "harassment": 0.8608233332633972,
    "violence": 0.6109251379966736,
    "self-harm": 0.95,
    "sexual": 0.6268884539604187,
    "shocking": 0.4719175909246717,
    "illegal activity": 0.5658302875452264,
    "deception": 0.6447434425354004,
    "political": 0.6043768728564599
  },
  "results": {
    "overall": {
      "acc": 0.6727093353064496,
      "prec": 0.5916245916245916,
      "rec": 0.6089880770406604,
      "f1": 0.600180777342573,
      "tp": 1992,
      "fp": 1375,
      "tn": 3463,
      "fn": 1279,
      "n": 8109
    },
    "cat::deception": {
      "acc": 0.7337461300309598,
      "prec": 0.6130268199233716,
      "rec": 0.6926406926406926,
      "f1": 0.6504065040650406,
      "tp": 160,
      "fp": 101,
      "tn": 314,
      "fn": 71,
      "n": 646
    },
    "cat::harassment": {
      "acc": 0.7573726541554959,
      "prec": 0.4659090909090909,
      "rec": 0.48520710059171596,
      "f1": 0.47536231884057967,
      "tp": 82,
      "fp": 94,
      "tn": 483,
      "fn": 87,
      "n": 746
    },
    "cat::hate": {
      "acc": 0.7989886219974716,
      "prec": 0.37037037037037035,
      "rec": 0.3053435114503817,
      "f1": 0.3347280334728034,
      "tp": 40,
      "fp": 68,
      "tn": 592,
      "fn": 91,
      "n": 791
    },
    "cat::illegal activity": {
      "acc": 0.5545851528384279,
      "prec": 0.5794117647058824,
      "rec": 0.5472222222222223,
      "f1": 0.5628571428571431,
      "tp": 197,
      "fp": 143,
      "tn": 184,
      "fn": 163,
      "n": 687
    },
    "cat::political": {
      "acc": 0.6626506024096386,
      "prec": 0.7239583333333334,
      "rec": 0.702020202020202,
      "f1": 0.7128205128205127,
      "tp": 278,
      "fp": 106,
      "tn": 162,
      "fn": 118,
      "n": 664
    },
    "cat::public and personal health": {
      "acc": 0.5456026058631922,
      "prec": 0.38589211618257263,
      "rec": 0.41517857142857145,
      "f1": 0.4,
      "tp": 93,
      "fp": 148,
      "tn": 242,
      "fn": 131,
      "n": 614
    },
    "cat::self-harm": {
      "acc": 0.6893081761006289,
      "prec": 0.21019108280254778,
      "rec": 0.21153846153846154,
      "f1": 0.2108626198083067,
      "tp": 33,
      "fp": 124,
      "tn": 515,
      "fn": 123,
      "n": 795
    },
    "cat::sexual": {
      "acc": 0.7212336892052195,
      "prec": 0.7342767295597484,
      "rec": 0.8761726078799249,
      "f1": 0.7989734816082121,
      "tp": 467,
      "fp": 169,
      "tn": 141,
      "fn": 66,
      "n": 843
    },
    "cat::shocking": {
      "acc": 0.6108695652173913,
      "prec": 0.6564102564102564,
      "rec": 0.5333333333333333,
      "f1": 0.5885057471264368,
      "tp": 256,
      "fp": 134,
      "tn": 306,
      "fn": 224,
      "n": 920
    },
    "cat::spam": {
      "acc": 0.6273584905660378,
      "prec": 0.5564853556485355,
      "rec": 0.5037878787878788,
      "f1": 0.5288270377733598,
      "tp": 133,
      "fp": 106,
      "tn": 266,
      "fn": 131,
      "n": 636
    },
    "cat::violence": {
      "acc": 0.666232073011734,
      "prec": 0.5816091954022988,
      "rec": 0.7737003058103975,
      "f1": 0.6640419947506562,
      "tp": 253,
      "fp": 182,
      "tn": 258,
      "fn": 74,
      "n": 767
    },
    "src::Laion5B": {
      "acc": 0.6751545117428924,
      "prec": 0.5382011605415861,
      "rec": 0.7561141304347826,
      "f1": 0.6288135593220339,
      "tp": 1113,
      "fp": 955,
      "tn": 1618,
      "fn": 359,
      "n": 4045
    },
    "src::Lexica": {
      "acc": 0.6702755905511811,
      "prec": 0.6766743648960739,
      "rec": 0.4886047804335742,
      "f1": 0.5674628792769529,
      "tp": 879,
      "fp": 420,
      "tn": 1845,
      "fn": 920,
      "n": 4064
    },
    "overall_probs": {
      "pr_auc": 0.5002651871190007,
      "roc_auc": 0.6491894710541445,
      "ece_15": 0.1990808818888648,
      "recall_at_1pct_fpr": 0.0012228676245796392,
      "recall_at_5pct_fpr": 0.05136044023234485,
      "fpr_at_90pct_recall": 0.7401818933443571,
      "fpr_at_95pct_recall": 0.8288548987184787
    },
    "cat_probs::deception": {
      "pr_auc": 0.6421732064065993,
      "roc_auc": 0.756386585302248,
      "ece_15": 0.1911335664256346
    },
    "cat_probs::harassment": {
      "pr_auc": 0.40073135659317055,
      "roc_auc": 0.70817224370084,
      "ece_15": 0.3826277811709422
    },
    "cat_probs::hate": {
      "pr_auc": 0.3241065087107435,
      "roc_auc": 0.713763590099468,
      "ece_15": 0.26219600586835223
    },
    "cat_probs::illegal activity": {
      "pr_auc": 0.6218200835303996,
      "roc_auc": 0.5813455657492355,
      "ece_15": 0.1499321136130169
    },
    "cat_probs::political": {
      "pr_auc": 0.6924591557188465,
      "roc_auc": 0.6847297602894618,
      "ece_15": 0.1089592794354736
    },
    "cat_probs::public and personal health": {
      "pr_auc": 0.35834378084546414,
      "roc_auc": 0.5203411172161172,
      "ece_15": 0.4500935684756076
    },
    "cat_probs::self-harm": {
      "pr_auc": 0.20388403561658033,
      "roc_auc": 0.4855342883511897,
      "ece_15": 0.45612237560709223
    },
    "cat_probs::sexual": {
      "pr_auc": 0.8169022576453677,
      "roc_auc": 0.7507413907885977,
      "ece_15": 0.11981925780830788
    },
    "cat_probs::shocking": {
      "pr_auc": 0.6298916838783597,
      "roc_auc": 0.6681202651515152,
      "ece_15": 0.10815216984964258
    },
    "cat_probs::spam": {
      "pr_auc": 0.48577914666901034,
      "roc_auc": 0.6256720430107527,
      "ece_15": 0.17693887481688514
    },
    "cat_probs::violence": {
      "pr_auc": 0.612158043972183,
      "roc_auc": 0.7299068668334724,
      "ece_15": 0.2085094999623967
    },
    "src_probs::Laion5B": {
      "pr_auc": 0.4683421384115505,
      "roc_auc": 0.678521413846128,
      "ece_15": 0.30649007841696035
    },
    "src_probs::Lexica": {
      "pr_auc": 0.5730830152590842,
      "roc_auc": 0.661490624543682,
      "ece_15": 0.1269371225701427
    }
  },
  "meta": {
    "duration_sec": 280.49523749999935,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 2781201408,
    "gpu_peak_reserved_bytes": 3430940672
  }
}