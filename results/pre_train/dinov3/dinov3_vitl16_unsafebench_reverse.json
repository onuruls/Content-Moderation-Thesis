{
  "thr": 0.5606707377532094,
  "thr_by_cat": {
    "public and personal health": 0.6639959792937002,
    "spam": 0.28930027037858963,
    "hate": 0.711232090415667,
    "harassment": 0.7933990955352783,
    "violence": 0.5911927346738509,
    "self-harm": 0.7019240260124207,
    "sexual": 0.4842578974945285,
    "shocking": 0.22213585132902322,
    "illegal activity": 0.28109589704247406,
    "deception": 0.6118125484033596,
    "political": 0.5929558841054312
  },
  "results": {
    "overall": {
      "acc": 0.6286841780737452,
      "prec": 0.5399262899262899,
      "rec": 0.5374503210027515,
      "f1": 0.5386854603952812,
      "tp": 1758,
      "fp": 1498,
      "tn": 3340,
      "fn": 1513,
      "n": 8109
    },
    "cat::deception": {
      "acc": 0.6160990712074303,
      "prec": 0.4669260700389105,
      "rec": 0.5194805194805194,
      "f1": 0.4918032786885246,
      "tp": 120,
      "fp": 137,
      "tn": 278,
      "fn": 111,
      "n": 646
    },
    "cat::harassment": {
      "acc": 0.6970509383378016,
      "prec": 0.3721973094170404,
      "rec": 0.4911242603550296,
      "f1": 0.42346938775510207,
      "tp": 83,
      "fp": 140,
      "tn": 437,
      "fn": 86,
      "n": 746
    },
    "cat::hate": {
      "acc": 0.7597977243994943,
      "prec": 0.14457831325301204,
      "rec": 0.0916030534351145,
      "f1": 0.11214953271028037,
      "tp": 12,
      "fp": 71,
      "tn": 589,
      "fn": 119,
      "n": 791
    },
    "cat::illegal activity": {
      "acc": 0.43231441048034935,
      "prec": 0.4583333333333333,
      "rec": 0.4583333333333333,
      "f1": 0.4583333333333333,
      "tp": 165,
      "fp": 195,
      "tn": 132,
      "fn": 195,
      "n": 687
    },
    "cat::political": {
      "acc": 0.6762048192771084,
      "prec": 0.7245657568238213,
      "rec": 0.7373737373737373,
      "f1": 0.7309136420525658,
      "tp": 292,
      "fp": 111,
      "tn": 157,
      "fn": 104,
      "n": 664
    },
    "cat::public and personal health": {
      "acc": 0.5879478827361564,
      "prec": 0.4349775784753363,
      "rec": 0.4330357142857143,
      "f1": 0.4340044742729306,
      "tp": 97,
      "fp": 126,
      "tn": 264,
      "fn": 127,
      "n": 614
    },
    "cat::self-harm": {
      "acc": 0.7383647798742138,
      "prec": 0.35714285714285715,
      "rec": 0.4166666666666667,
      "f1": 0.3846153846153846,
      "tp": 65,
      "fp": 117,
      "tn": 522,
      "fn": 91,
      "n": 795
    },
    "cat::sexual": {
      "acc": 0.6429418742586003,
      "prec": 0.7078853046594982,
      "rec": 0.7410881801125704,
      "f1": 0.7241063244729606,
      "tp": 395,
      "fp": 163,
      "tn": 147,
      "fn": 138,
      "n": 843
    },
    "cat::shocking": {
      "acc": 0.4826086956521739,
      "prec": 0.5044444444444445,
      "rec": 0.47291666666666665,
      "f1": 0.4881720430107527,
      "tp": 227,
      "fp": 223,
      "tn": 217,
      "fn": 253,
      "n": 920
    },
    "cat::spam": {
      "acc": 0.5943396226415094,
      "prec": 0.5176470588235295,
      "rec": 0.3333333333333333,
      "f1": 0.40552995391705066,
      "tp": 88,
      "fp": 82,
      "tn": 290,
      "fn": 176,
      "n": 636
    },
    "cat::violence": {
      "acc": 0.6792698826597132,
      "prec": 0.6167146974063401,
      "rec": 0.654434250764526,
      "f1": 0.6350148367952523,
      "tp": 214,
      "fp": 133,
      "tn": 307,
      "fn": 113,
      "n": 767
    },
    "src::Laion5B": {
      "acc": 0.6405438813349814,
      "prec": 0.5056818181818182,
      "rec": 0.5441576086956522,
      "f1": 0.5242146596858639,
      "tp": 801,
      "fp": 783,
      "tn": 1790,
      "fn": 671,
      "n": 4045
    },
    "src::Lexica": {
      "acc": 0.6168799212598425,
      "prec": 0.5723684210526315,
      "rec": 0.5319622012229016,
      "f1": 0.5514261019878998,
      "tp": 957,
      "fp": 715,
      "tn": 1550,
      "fn": 842,
      "n": 4064
    },
    "overall_probs": {
      "pr_auc": 0.4957935741057468,
      "roc_auc": 0.6085137671817261,
      "ece_15": 0.20534467371019866,
      "recall_at_1pct_fpr": 0.013757260776520942,
      "recall_at_5pct_fpr": 0.10486089880770406,
      "fpr_at_90pct_recall": 0.8178999586606036,
      "fpr_at_95pct_recall": 0.8954113269946259
    },
    "cat_probs::deception": {
      "pr_auc": 0.5114751924001921,
      "roc_auc": 0.61736817399468,
      "ece_15": 0.25779678936333733
    },
    "cat_probs::harassment": {
      "pr_auc": 0.40104502340271275,
      "roc_auc": 0.7067775578640796,
      "ece_15": 0.3127311617966202
    },
    "cat_probs::hate": {
      "pr_auc": 0.1687492411477683,
      "roc_auc": 0.5287531806615776,
      "ece_15": 0.2238204277351742
    },
    "cat_probs::illegal activity": {
      "pr_auc": 0.48759345373725593,
      "roc_auc": 0.4426945293917771,
      "ece_15": 0.3287195609997703
    },
    "cat_probs::political": {
      "pr_auc": 0.6957106950011072,
      "roc_auc": 0.6895918136589779,
      "ece_15": 0.12520800491751813
    },
    "cat_probs::public and personal health": {
      "pr_auc": 0.3767302878098123,
      "roc_auc": 0.5596497252747252,
      "ece_15": 0.23272674078582894
    },
    "cat_probs::self-harm": {
      "pr_auc": 0.3423936926213063,
      "roc_auc": 0.6933810842261546,
      "ece_15": 0.23838782615596485
    },
    "cat_probs::sexual": {
      "pr_auc": 0.754354566614908,
      "roc_auc": 0.6546329359075228,
      "ece_15": 0.15353555382259212
    },
    "cat_probs::shocking": {
      "pr_auc": 0.5059704377861576,
      "roc_auc": 0.4890435606060606,
      "ece_15": 0.3229990060222285
    },
    "cat_probs::spam": {
      "pr_auc": 0.4696719886237749,
      "roc_auc": 0.6179537308569566,
      "ece_15": 0.3051603870974662
    },
    "cat_probs::violence": {
      "pr_auc": 0.6099228284369227,
      "roc_auc": 0.7306783430636641,
      "ece_15": 0.13194528393701782
    },
    "src_probs::Laion5B": {
      "pr_auc": 0.4822572469955636,
      "roc_auc": 0.640856553845114,
      "ece_15": 0.20090678796046663
    },
    "src_probs::Lexica": {
      "pr_auc": 0.5131978109211541,
      "roc_auc": 0.5763766232650712,
      "ece_15": 0.22122586633537772
    }
  },
  "meta": {
    "duration_sec": 292.6529838999995,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 4834430976,
    "gpu_peak_reserved_bytes": 5666504704
  }
}