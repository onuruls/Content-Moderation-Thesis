{
  "thr": 0.6415772161532923,
  "thr_by_cat": {
    "public and personal health": 0.6298794746398926,
    "spam": 0.5911407977342605,
    "hate": 0.631924548640323,
    "harassment": 0.7207320303840433,
    "violence": 0.6278724074363708,
    "self-harm": 0.7201866878936038,
    "sexual": 0.6844029245783367,
    "shocking": 0.6435540512010649,
    "illegal activity": 0.5834229907324148,
    "deception": 0.5950727751649962,
    "political": 0.5795250592117538
  },
  "results": {
    "overall": {
      "acc": 0.6193118756936737,
      "prec": 0.5273159144893111,
      "rec": 0.5429532253133599,
      "f1": 0.535020334387709,
      "tp": 1776,
      "fp": 1592,
      "tn": 3246,
      "fn": 1495,
      "n": 8109
    },
    "cat::deception": {
      "acc": 0.5108359133126935,
      "prec": 0.2985781990521327,
      "rec": 0.2727272727272727,
      "f1": 0.2850678733031674,
      "tp": 63,
      "fp": 148,
      "tn": 267,
      "fn": 168,
      "n": 646
    },
    "cat::harassment": {
      "acc": 0.725201072386059,
      "prec": 0.3783783783783784,
      "rec": 0.33136094674556216,
      "f1": 0.3533123028391168,
      "tp": 56,
      "fp": 92,
      "tn": 485,
      "fn": 113,
      "n": 746
    },
    "cat::hate": {
      "acc": 0.7104930467762326,
      "prec": 0.16891891891891891,
      "rec": 0.19083969465648856,
      "f1": 0.17921146953405018,
      "tp": 25,
      "fp": 123,
      "tn": 537,
      "fn": 106,
      "n": 791
    },
    "cat::illegal activity": {
      "acc": 0.512372634643377,
      "prec": 0.53315649867374,
      "rec": 0.5583333333333333,
      "f1": 0.5454545454545455,
      "tp": 201,
      "fp": 176,
      "tn": 151,
      "fn": 159,
      "n": 687
    },
    "cat::political": {
      "acc": 0.5783132530120482,
      "prec": 0.6542553191489362,
      "rec": 0.6212121212121212,
      "f1": 0.6373056994818653,
      "tp": 246,
      "fp": 130,
      "tn": 138,
      "fn": 150,
      "n": 664
    },
    "cat::public and personal health": {
      "acc": 0.5977198697068404,
      "prec": 0.4602076124567474,
      "rec": 0.59375,
      "f1": 0.5185185185185186,
      "tp": 133,
      "fp": 156,
      "tn": 234,
      "fn": 91,
      "n": 614
    },
    "cat::self-harm": {
      "acc": 0.710691823899371,
      "prec": 0.2109375,
      "rec": 0.17307692307692307,
      "f1": 0.19014084507042256,
      "tp": 27,
      "fp": 101,
      "tn": 538,
      "fn": 129,
      "n": 795
    },
    "cat::sexual": {
      "acc": 0.641755634638197,
      "prec": 0.7073608617594255,
      "rec": 0.7392120075046904,
      "f1": 0.7229357798165137,
      "tp": 394,
      "fp": 163,
      "tn": 147,
      "fn": 139,
      "n": 843
    },
    "cat::shocking": {
      "acc": 0.625,
      "prec": 0.6403326403326404,
      "rec": 0.6416666666666667,
      "f1": 0.6409989594172737,
      "tp": 308,
      "fp": 173,
      "tn": 267,
      "fn": 172,
      "n": 920
    },
    "cat::spam": {
      "acc": 0.5094339622641509,
      "prec": 0.4016393442622951,
      "rec": 0.3712121212121212,
      "f1": 0.38582677165354334,
      "tp": 98,
      "fp": 146,
      "tn": 226,
      "fn": 166,
      "n": 636
    },
    "cat::violence": {
      "acc": 0.6271186440677966,
      "prec": 0.5501222493887531,
      "rec": 0.6880733944954128,
      "f1": 0.6114130434782609,
      "tp": 225,
      "fp": 184,
      "tn": 256,
      "fn": 102,
      "n": 767
    },
    "src::Laion5B": {
      "acc": 0.5950556242274413,
      "prec": 0.4399421128798842,
      "rec": 0.41304347826086957,
      "f1": 0.4260686755430974,
      "tp": 608,
      "fp": 774,
      "tn": 1799,
      "fn": 864,
      "n": 4045
    },
    "src::Lexica": {
      "acc": 0.6434547244094488,
      "prec": 0.5881168177240684,
      "rec": 0.6492495831017232,
      "f1": 0.6171730515191546,
      "tp": 1168,
      "fp": 818,
      "tn": 1447,
      "fn": 631,
      "n": 4064
    },
    "overall_probs": {
      "pr_auc": 0.506571954906311,
      "roc_auc": 0.6115668288436508,
      "ece_15": 0.22000943977863083,
      "recall_at_1pct_fpr": 0.022623051054723325,
      "recall_at_5pct_fpr": 0.10302659737083461,
      "fpr_at_90pct_recall": 0.850558081852005,
      "fpr_at_95pct_recall": 0.9264158743282348
    },
    "cat_probs::deception": {
      "pr_auc": 0.309885405719157,
      "roc_auc": 0.42934334741563657,
      "ece_15": 0.21720419856416923
    },
    "cat_probs::harassment": {
      "pr_auc": 0.33765754295175476,
      "roc_auc": 0.6905028047542379,
      "ece_15": 0.4084892436742143
    },
    "cat_probs::hate": {
      "pr_auc": 0.17289984024081953,
      "roc_auc": 0.4795743696507055,
      "ece_15": 0.40268098965155336
    },
    "cat_probs::illegal activity": {
      "pr_auc": 0.5404919690483828,
      "roc_auc": 0.5226554536187563,
      "ece_15": 0.08436250751716085
    },
    "cat_probs::political": {
      "pr_auc": 0.69310991173335,
      "roc_auc": 0.5993140358812,
      "ece_15": 0.024977114994123768
    },
    "cat_probs::public and personal health": {
      "pr_auc": 0.4306706040194784,
      "roc_auc": 0.6152815934065934,
      "ece_15": 0.2615800465939488
    },
    "cat_probs::self-harm": {
      "pr_auc": 0.240463301414626,
      "roc_auc": 0.6155852493880662,
      "ece_15": 0.4331791174111876
    },
    "cat_probs::sexual": {
      "pr_auc": 0.7267267202742994,
      "roc_auc": 0.6363553834049507,
      "ece_15": 0.11758774377555892
    },
    "cat_probs::shocking": {
      "pr_auc": 0.6473379570361368,
      "roc_auc": 0.6697301136363637,
      "ece_15": 0.13218112234836044
    },
    "cat_probs::spam": {
      "pr_auc": 0.4244293528507469,
      "roc_auc": 0.5165974258716194,
      "ece_15": 0.16682931958879313
    },
    "cat_probs::violence": {
      "pr_auc": 0.5915391462551922,
      "roc_auc": 0.6653113705865998,
      "ece_15": 0.2101432666318513
    },
    "src_probs::Laion5B": {
      "pr_auc": 0.41253504870928104,
      "roc_auc": 0.547682402118995,
      "ece_15": 0.2437603711418817
    },
    "src_probs::Lexica": {
      "pr_auc": 0.5791147017176109,
      "roc_auc": 0.6568992830208591,
      "ece_15": 0.1963695486948833
    }
  },
  "meta": {
    "duration_sec": 611.248080100002,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 0,
    "gpu_peak_reserved_bytes": 0
  }
}