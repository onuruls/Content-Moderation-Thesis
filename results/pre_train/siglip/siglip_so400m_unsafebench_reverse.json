{
  "thr": 0.9409722200374013,
  "thr_by_cat": {
    "public and personal health": 0.95,
    "spam": 0.920203223824501,
    "hate": 0.95,
    "harassment": 0.95,
    "violence": 0.881564485903231,
    "self-harm": 0.95,
    "sexual": 0.9127619984025639,
    "shocking": 0.8750078693612829,
    "illegal activity": 0.8326016387274099,
    "deception": 0.9171880074073931,
    "political": 0.95
  },
  "results": {
    "overall": {
      "acc": 0.6601307189542484,
      "prec": 0.5695382122603294,
      "rec": 0.6447569550596148,
      "f1": 0.6048178950387153,
      "tp": 2109,
      "fp": 1594,
      "tn": 3244,
      "fn": 1162,
      "n": 8109
    },
    "cat::deception": {
      "acc": 0.6749226006191951,
      "prec": 0.5475113122171946,
      "rec": 0.5238095238095238,
      "f1": 0.5353982300884955,
      "tp": 121,
      "fp": 100,
      "tn": 315,
      "fn": 110,
      "n": 646
    },
    "cat::harassment": {
      "acc": 0.6420911528150134,
      "prec": 0.3496932515337423,
      "rec": 0.6745562130177515,
      "f1": 0.4606060606060606,
      "tp": 114,
      "fp": 212,
      "tn": 365,
      "fn": 55,
      "n": 746
    },
    "cat::hate": {
      "acc": 0.7155499367888748,
      "prec": 0.24175824175824176,
      "rec": 0.33587786259541985,
      "f1": 0.28115015974440893,
      "tp": 44,
      "fp": 138,
      "tn": 522,
      "fn": 87,
      "n": 791
    },
    "cat::illegal activity": {
      "acc": 0.5516739446870451,
      "prec": 0.5653266331658291,
      "rec": 0.625,
      "f1": 0.5936675461741424,
      "tp": 225,
      "fp": 173,
      "tn": 154,
      "fn": 135,
      "n": 687
    },
    "cat::political": {
      "acc": 0.7394578313253012,
      "prec": 0.7252525252525253,
      "rec": 0.9065656565656566,
      "f1": 0.8058361391694725,
      "tp": 359,
      "fp": 136,
      "tn": 132,
      "fn": 37,
      "n": 664
    },
    "cat::public and personal health": {
      "acc": 0.5260586319218241,
      "prec": 0.3864406779661017,
      "rec": 0.5089285714285714,
      "f1": 0.4393063583815029,
      "tp": 114,
      "fp": 181,
      "tn": 209,
      "fn": 110,
      "n": 614
    },
    "cat::self-harm": {
      "acc": 0.7132075471698113,
      "prec": 0.36666666666666664,
      "rec": 0.6346153846153846,
      "f1": 0.46478873239436613,
      "tp": 99,
      "fp": 171,
      "tn": 468,
      "fn": 57,
      "n": 795
    },
    "cat::sexual": {
      "acc": 0.6868327402135231,
      "prec": 0.7717171717171717,
      "rec": 0.7166979362101313,
      "f1": 0.7431906614785992,
      "tp": 382,
      "fp": 113,
      "tn": 197,
      "fn": 151,
      "n": 843
    },
    "cat::shocking": {
      "acc": 0.6532608695652173,
      "prec": 0.673866090712743,
      "rec": 0.65,
      "f1": 0.6617179215270413,
      "tp": 312,
      "fp": 151,
      "tn": 289,
      "fn": 168,
      "n": 920
    },
    "cat::spam": {
      "acc": 0.6918238993710691,
      "prec": 0.636,
      "rec": 0.6022727272727273,
      "f1": 0.6186770428015566,
      "tp": 159,
      "fp": 91,
      "tn": 281,
      "fn": 105,
      "n": 636
    },
    "cat::violence": {
      "acc": 0.6414602346805737,
      "prec": 0.5844155844155844,
      "rec": 0.5504587155963303,
      "f1": 0.5669291338582677,
      "tp": 180,
      "fp": 128,
      "tn": 312,
      "fn": 147,
      "n": 767
    },
    "src::Laion5B": {
      "acc": 0.6608158220024722,
      "prec": 0.525,
      "rec": 0.7133152173913043,
      "f1": 0.6048387096774194,
      "tp": 1050,
      "fp": 950,
      "tn": 1623,
      "fn": 422,
      "n": 4045
    },
    "src::Lexica": {
      "acc": 0.6594488188976378,
      "prec": 0.6218438050499119,
      "rec": 0.5886603668704836,
      "f1": 0.604797258709309,
      "tp": 1059,
      "fp": 644,
      "tn": 1621,
      "fn": 740,
      "n": 4064
    },
    "overall_probs": {
      "pr_auc": 0.548849136837356,
      "roc_auc": 0.6796323788958526,
      "ece_15": 0.3979879292569993,
      "recall_at_1pct_fpr": 0.013451543870376031,
      "recall_at_5pct_fpr": 0.12992968511158667,
      "fpr_at_90pct_recall": 0.7046300124018189,
      "fpr_at_95pct_recall": 0.812525837122778
    },
    "cat_probs::deception": {
      "pr_auc": 0.5981803131218925,
      "roc_auc": 0.7088822823762584,
      "ece_15": 0.41454931153647673
    },
    "cat_probs::harassment": {
      "pr_auc": 0.4809769414157705,
      "roc_auc": 0.7130638991724181,
      "ece_15": 0.5286135461603368
    },
    "cat_probs::hate": {
      "pr_auc": 0.2153136363318438,
      "roc_auc": 0.5472819801064076,
      "ece_15": 0.6360383118901934
    },
    "cat_probs::illegal activity": {
      "pr_auc": 0.6130183638454136,
      "roc_auc": 0.5816343866802582,
      "ece_15": 0.277002282852776
    },
    "cat_probs::political": {
      "pr_auc": 0.7125366050547238,
      "roc_auc": 0.7103403437358661,
      "ece_15": 0.28800044601351155
    },
    "cat_probs::public and personal health": {
      "pr_auc": 0.37778183246945646,
      "roc_auc": 0.5519803113553113,
      "ece_15": 0.48355474450446495
    },
    "cat_probs::self-harm": {
      "pr_auc": 0.4455955833993834,
      "roc_auc": 0.7339492797239275,
      "ece_15": 0.6277714719749846
    },
    "cat_probs::sexual": {
      "pr_auc": 0.7886101827334151,
      "roc_auc": 0.724614174181444,
      "ece_15": 0.23748585061652944
    },
    "cat_probs::shocking": {
      "pr_auc": 0.6361909286381877,
      "roc_auc": 0.6917140151515152,
      "ece_15": 0.285988559063686
    },
    "cat_probs::spam": {
      "pr_auc": 0.5455137712405635,
      "roc_auc": 0.7286066308243727,
      "ece_15": 0.3550979299646503
    },
    "cat_probs::violence": {
      "pr_auc": 0.597177467721872,
      "roc_auc": 0.6936613844870727,
      "ece_15": 0.28667524580101844
    },
    "src_probs::Laion5B": {
      "pr_auc": 0.5034253239181989,
      "roc_auc": 0.6932781529343179,
      "ece_15": 0.4516811397743995
    },
    "src_probs::Lexica": {
      "pr_auc": 0.6247878558519496,
      "roc_auc": 0.6803678030595854,
      "ece_15": 0.3455429200101809
    }
  },
  "meta": {
    "duration_sec": 453.8185830000002,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 3724899328,
    "gpu_peak_reserved_bytes": 4599054336
  }
}