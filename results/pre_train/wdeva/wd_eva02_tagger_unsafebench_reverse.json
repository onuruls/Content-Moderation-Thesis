{
  "thr": 0.16526055090206188,
  "thr_by_cat": {
    "public and personal health": 0.1606878465221774,
    "spam": 0.05,
    "hate": 0.13787259049152006,
    "harassment": 0.5003642525902406,
    "violence": 0.10052490234375,
    "self-harm": 0.4778433181532663,
    "sexual": 0.417983791839455,
    "shocking": 0.1911874746347403,
    "illegal activity": 0.05407005132630814,
    "deception": 0.05,
    "political": 0.05
  },
  "results": {
    "overall": {
      "acc": 0.6353434455543223,
      "prec": 0.5479828850855746,
      "rec": 0.5481504127178233,
      "f1": 0.5480666360996485,
      "tp": 1793,
      "fp": 1479,
      "tn": 3359,
      "fn": 1478,
      "n": 8109
    },
    "cat::deception": {
      "acc": 0.5928792569659442,
      "prec": 0.41578947368421054,
      "rec": 0.341991341991342,
      "f1": 0.3752969121140143,
      "tp": 79,
      "fp": 111,
      "tn": 304,
      "fn": 152,
      "n": 646
    },
    "cat::harassment": {
      "acc": 0.6997319034852547,
      "prec": 0.2727272727272727,
      "rec": 0.1952662721893491,
      "f1": 0.2275862068965517,
      "tp": 33,
      "fp": 88,
      "tn": 489,
      "fn": 136,
      "n": 746
    },
    "cat::hate": {
      "acc": 0.7547408343868521,
      "prec": 0.2765957446808511,
      "rec": 0.29770992366412213,
      "f1": 0.286764705882353,
      "tp": 39,
      "fp": 102,
      "tn": 558,
      "fn": 92,
      "n": 791
    },
    "cat::illegal activity": {
      "acc": 0.4861717612809316,
      "prec": 0.5088607594936709,
      "rec": 0.5583333333333333,
      "f1": 0.5324503311258278,
      "tp": 201,
      "fp": 194,
      "tn": 133,
      "fn": 159,
      "n": 687
    },
    "cat::political": {
      "acc": 0.5993975903614458,
      "prec": 0.6805555555555556,
      "rec": 0.6186868686868687,
      "f1": 0.6481481481481481,
      "tp": 245,
      "fp": 115,
      "tn": 153,
      "fn": 151,
      "n": 664
    },
    "cat::public and personal health": {
      "acc": 0.5879478827361564,
      "prec": 0.43612334801762115,
      "rec": 0.4419642857142857,
      "f1": 0.43902439024390244,
      "tp": 99,
      "fp": 128,
      "tn": 262,
      "fn": 125,
      "n": 614
    },
    "cat::self-harm": {
      "acc": 0.7396226415094339,
      "prec": 0.3082706766917293,
      "rec": 0.26282051282051283,
      "f1": 0.28373702422145325,
      "tp": 41,
      "fp": 92,
      "tn": 547,
      "fn": 115,
      "n": 795
    },
    "cat::sexual": {
      "acc": 0.6666666666666666,
      "prec": 0.7058823529411765,
      "rec": 0.8105065666041276,
      "f1": 0.754585152838428,
      "tp": 432,
      "fp": 180,
      "tn": 130,
      "fn": 101,
      "n": 843
    },
    "cat::shocking": {
      "acc": 0.6652173913043479,
      "prec": 0.6963470319634704,
      "rec": 0.6354166666666666,
      "f1": 0.664488017429194,
      "tp": 305,
      "fp": 133,
      "tn": 307,
      "fn": 175,
      "n": 920
    },
    "cat::spam": {
      "acc": 0.5188679245283019,
      "prec": 0.4132231404958678,
      "rec": 0.3787878787878788,
      "f1": 0.3952569169960474,
      "tp": 100,
      "fp": 142,
      "tn": 230,
      "fn": 164,
      "n": 636
    },
    "cat::violence": {
      "acc": 0.60625814863103,
      "prec": 0.5302663438256658,
      "rec": 0.6697247706422018,
      "f1": 0.5918918918918918,
      "tp": 219,
      "fp": 194,
      "tn": 246,
      "fn": 108,
      "n": 767
    },
    "src::Laion5B": {
      "acc": 0.6373300370828183,
      "prec": 0.501802451333814,
      "rec": 0.47282608695652173,
      "f1": 0.4868835257082896,
      "tp": 696,
      "fp": 691,
      "tn": 1882,
      "fn": 776,
      "n": 4045
    },
    "src::Lexica": {
      "acc": 0.6333661417322834,
      "prec": 0.5819628647214854,
      "rec": 0.6097832128960534,
      "f1": 0.5955483170466884,
      "tp": 1097,
      "fp": 788,
      "tn": 1477,
      "fn": 702,
      "n": 4064
    },
    "overall_probs": {
      "pr_auc": 0.5188170013901201,
      "roc_auc": 0.6256010547296453,
      "ece_15": 0.24206658564567682,
      "recall_at_1pct_fpr": 0.03699174564353409,
      "recall_at_5pct_fpr": 0.11066952002445735,
      "fpr_at_90pct_recall": 0.7949565936337329,
      "fpr_at_95pct_recall": 0.8792889623811493
    },
    "cat_probs::deception": {
      "pr_auc": 0.3756971703753378,
      "roc_auc": 0.5307463620716633,
      "ece_15": 0.31621545018057334
    },
    "cat_probs::harassment": {
      "pr_auc": 0.2900659613932976,
      "roc_auc": 0.6396890670987457,
      "ece_15": 0.15376847167436944
    },
    "cat_probs::hate": {
      "pr_auc": 0.24947066082651517,
      "roc_auc": 0.6167996761508212,
      "ece_15": 0.12241068862934329
    },
    "cat_probs::illegal activity": {
      "pr_auc": 0.5132465155903971,
      "roc_auc": 0.4934505606523955,
      "ece_15": 0.3908914069074384
    },
    "cat_probs::political": {
      "pr_auc": 0.6152753197899123,
      "roc_auc": 0.6019288029549223,
      "ece_15": 0.4931856350726392
    },
    "cat_probs::public and personal health": {
      "pr_auc": 0.3980390455620665,
      "roc_auc": 0.5555002289377289,
      "ece_15": 0.2491344302795609
    },
    "cat_probs::self-harm": {
      "pr_auc": 0.27234665305724237,
      "roc_auc": 0.6260784077685486,
      "ece_15": 0.14906072796515699
    },
    "cat_probs::sexual": {
      "pr_auc": 0.7488746155862418,
      "roc_auc": 0.6565726562972826,
      "ece_15": 0.164935117655943
    },
    "cat_probs::shocking": {
      "pr_auc": 0.6953553863145413,
      "roc_auc": 0.71890625,
      "ece_15": 0.2840685077335523
    },
    "cat_probs::spam": {
      "pr_auc": 0.4167297846046968,
      "roc_auc": 0.5448028673835126,
      "ece_15": 0.3786303502208782
    },
    "cat_probs::violence": {
      "pr_auc": 0.5528026057361065,
      "roc_auc": 0.64236169029747,
      "ece_15": 0.23353571189273603
    },
    "src_probs::Laion5B": {
      "pr_auc": 0.44841970964762823,
      "roc_auc": 0.5988107848645634,
      "ece_15": 0.23388631346788633
    },
    "src_probs::Lexica": {
      "pr_auc": 0.5785606092228277,
      "roc_auc": 0.6421074008493803,
      "ece_15": 0.2502086134407464
    }
  },
  "meta": {
    "duration_sec": 631.6804859000003,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 3155956224,
    "gpu_peak_reserved_bytes": 6601834496
  }
}