{
  "thr": 0.23828125,
  "thr_by_cat": {
    "hate": 0.21880987772597982,
    "violence": 0.21611649613958606,
    "self-harm": 0.7143960053066052,
    "sexual": 0.7688634109949585,
    "shocking": 0.37928838315217395,
    "illegal activity": 0.1,
    "deception": 0.05,
    "political": 0.05,
    "public and personal health": 0.4194630178644138,
    "spam": 0.1,
    "harassment": 0.7080078125
  },
  "results": {
    "overall": {
      "acc": 0.6494845360824743,
      "prec": 0.5417218543046357,
      "rec": 0.5263835263835264,
      "f1": 0.5339425587467364,
      "tp": 409,
      "fp": 346,
      "tn": 914,
      "fn": 368,
      "n": 2037
    },
    "cat::deception": {
      "acc": 0.588957055214724,
      "prec": 0.32653061224489793,
      "rec": 0.32,
      "f1": 0.3232323232323232,
      "tp": 16,
      "fp": 33,
      "tn": 80,
      "fn": 34,
      "n": 163
    },
    "cat::harassment": {
      "acc": 0.7165775401069518,
      "prec": 0.3488372093023256,
      "rec": 0.375,
      "f1": 0.3614457831325302,
      "tp": 15,
      "fp": 28,
      "tn": 119,
      "fn": 25,
      "n": 187
    },
    "cat::hate": {
      "acc": 0.7336683417085427,
      "prec": 0.1,
      "rec": 0.10344827586206896,
      "f1": 0.10169491525423728,
      "tp": 3,
      "fp": 27,
      "tn": 143,
      "fn": 26,
      "n": 199
    },
    "cat::illegal activity": {
      "acc": 0.5406976744186046,
      "prec": 0.5633802816901409,
      "rec": 0.45454545454545453,
      "f1": 0.5031446540880503,
      "tp": 40,
      "fp": 31,
      "tn": 53,
      "fn": 48,
      "n": 172
    },
    "cat::political": {
      "acc": 0.6167664670658682,
      "prec": 0.6753246753246753,
      "rec": 0.5714285714285714,
      "f1": 0.619047619047619,
      "tp": 52,
      "fp": 25,
      "tn": 51,
      "fn": 39,
      "n": 167
    },
    "cat::public and personal health": {
      "acc": 0.6,
      "prec": 0.4444444444444444,
      "rec": 0.509090909090909,
      "f1": 0.47457627118644063,
      "tp": 28,
      "fp": 35,
      "tn": 65,
      "fn": 27,
      "n": 155
    },
    "cat::self-harm": {
      "acc": 0.7336683417085427,
      "prec": 0.16666666666666666,
      "rec": 0.20689655172413793,
      "f1": 0.18461538461538463,
      "tp": 6,
      "fp": 30,
      "tn": 140,
      "fn": 23,
      "n": 199
    },
    "cat::sexual": {
      "acc": 0.6445497630331753,
      "prec": 0.7697841726618705,
      "rec": 0.7133333333333334,
      "f1": 0.740484429065744,
      "tp": 107,
      "fp": 32,
      "tn": 29,
      "fn": 43,
      "n": 211
    },
    "cat::shocking": {
      "acc": 0.6406926406926406,
      "prec": 0.6347826086956522,
      "rec": 0.6403508771929824,
      "f1": 0.6375545851528385,
      "tp": 73,
      "fp": 42,
      "tn": 75,
      "fn": 41,
      "n": 231
    },
    "cat::spam": {
      "acc": 0.64375,
      "prec": 0.44680851063829785,
      "rec": 0.40384615384615385,
      "f1": 0.42424242424242425,
      "tp": 21,
      "fp": 26,
      "tn": 82,
      "fn": 31,
      "n": 160
    },
    "cat::violence": {
      "acc": 0.6476683937823834,
      "prec": 0.5647058823529412,
      "rec": 0.6075949367088608,
      "f1": 0.5853658536585367,
      "tp": 48,
      "fp": 37,
      "tn": 77,
      "fn": 31,
      "n": 193
    },
    "src::Laion5B": {
      "acc": 0.6334975369458128,
      "prec": 0.4818181818181818,
      "rec": 0.44166666666666665,
      "f1": 0.4608695652173913,
      "tp": 159,
      "fp": 171,
      "tn": 484,
      "fn": 201,
      "n": 1015
    },
    "src::Lexica": {
      "acc": 0.6653620352250489,
      "prec": 0.5882352941176471,
      "rec": 0.5995203836930456,
      "f1": 0.5938242280285037,
      "tp": 250,
      "fp": 175,
      "tn": 430,
      "fn": 167,
      "n": 1022
    },
    "overall_probs": {
      "pr_auc": 0.5251156696136965,
      "roc_auc": 0.646236542665114,
      "ece_15": 0.22571928031782806,
      "recall_at_1pct_fpr": 0.036036036036036036,
      "recall_at_5pct_fpr": 0.12998712998713,
      "fpr_at_90pct_recall": 0.7642857142857142,
      "fpr_at_95pct_recall": 0.8928571428571429
    },
    "cat_probs::deception": {
      "pr_auc": 0.3432242984551815,
      "roc_auc": 0.6038053097345132,
      "ece_15": 0.3021911550884597
    },
    "cat_probs::harassment": {
      "pr_auc": 0.3316866853807531,
      "roc_auc": 0.7040816326530613,
      "ece_15": 0.20784181197059348
    },
    "cat_probs::hate": {
      "pr_auc": 0.13366347945529647,
      "roc_auc": 0.4721095334685598,
      "ece_15": 0.18958088860439892
    },
    "cat_probs::illegal activity": {
      "pr_auc": 0.5387547645304754,
      "roc_auc": 0.5675730519480519,
      "ece_15": 0.4296114056609398
    },
    "cat_probs::political": {
      "pr_auc": 0.5922578348056955,
      "roc_auc": 0.6008530942741469,
      "ece_15": 0.46117863398112224
    },
    "cat_probs::public and personal health": {
      "pr_auc": 0.4014220560927387,
      "roc_auc": 0.5868181818181818,
      "ece_15": 0.2513078935684696
    },
    "cat_probs::self-harm": {
      "pr_auc": 0.1971394211999455,
      "roc_auc": 0.6304259634888438,
      "ece_15": 0.22574314519987632
    },
    "cat_probs::sexual": {
      "pr_auc": 0.8140258442116598,
      "roc_auc": 0.6485245901639345,
      "ece_15": 0.21531319279241334
    },
    "cat_probs::shocking": {
      "pr_auc": 0.6322078226382757,
      "roc_auc": 0.676488229119808,
      "ece_15": 0.19713147481282556
    },
    "cat_probs::spam": {
      "pr_auc": 0.417084210505277,
      "roc_auc": 0.5820868945868946,
      "ece_15": 0.29081594944000244
    },
    "cat_probs::violence": {
      "pr_auc": 0.6034429988489249,
      "roc_auc": 0.6977570508549856,
      "ece_15": 0.22369666914865763
    },
    "src_probs::Laion5B": {
      "pr_auc": 0.46180874801999466,
      "roc_auc": 0.5963125530110263,
      "ece_15": 0.24384104395147613
    },
    "src_probs::Lexica": {
      "pr_auc": 0.5806883680959005,
      "roc_auc": 0.6858711378005033,
      "ece_15": 0.21156936149074604
    }
  },
  "meta": {
    "duration_sec": 583.7522815999982,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 3154177024,
    "gpu_peak_reserved_bytes": 6144655360
  }
}