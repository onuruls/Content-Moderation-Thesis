{
  "thr": 0.5017559159662306,
  "thr_by_cat": {
    "public and personal health": 0.7950490059391145,
    "spam": 0.16927626803517343,
    "hate": 0.28983086347579956,
    "harassment": 0.9473617564548146,
    "violence": 0.49455798768626597,
    "self-harm": 0.6987417936325073,
    "sexual": 0.368088322228165,
    "shocking": 0.25865659582150446,
    "illegal activity": 0.5536272040633268,
    "deception": 0.3722929349340544,
    "political": 0.7679078775251698
  },
  "results": {
    "overall": {
      "acc": 0.6485386607473178,
      "prec": 0.5674463313040692,
      "rec": 0.5414246407826353,
      "f1": 0.5541301627033792,
      "tp": 1771,
      "fp": 1350,
      "tn": 3488,
      "fn": 1500,
      "n": 8109
    },
    "cat::deception": {
      "acc": 0.631578947368421,
      "prec": 0.4859437751004016,
      "rec": 0.5238095238095238,
      "f1": 0.5041666666666668,
      "tp": 121,
      "fp": 128,
      "tn": 287,
      "fn": 110,
      "n": 646
    },
    "cat::harassment": {
      "acc": 0.7935656836461126,
      "prec": 0.5688073394495413,
      "rec": 0.3668639053254438,
      "f1": 0.4460431654676259,
      "tp": 62,
      "fp": 47,
      "tn": 530,
      "fn": 107,
      "n": 746
    },
    "cat::hate": {
      "acc": 0.7029077117572693,
      "prec": 0.19767441860465115,
      "rec": 0.2595419847328244,
      "f1": 0.2244224422442244,
      "tp": 34,
      "fp": 138,
      "tn": 522,
      "fn": 97,
      "n": 791
    },
    "cat::illegal activity": {
      "acc": 0.529839883551674,
      "prec": 0.5493333333333333,
      "rec": 0.5722222222222222,
      "f1": 0.5605442176870747,
      "tp": 206,
      "fp": 169,
      "tn": 158,
      "fn": 154,
      "n": 687
    },
    "cat::political": {
      "acc": 0.6987951807228916,
      "prec": 0.756544502617801,
      "rec": 0.7297979797979798,
      "f1": 0.7429305912596402,
      "tp": 289,
      "fp": 93,
      "tn": 175,
      "fn": 107,
      "n": 664
    },
    "cat::public and personal health": {
      "acc": 0.5553745928338762,
      "prec": 0.3920704845814978,
      "rec": 0.39732142857142855,
      "f1": 0.3946784922394678,
      "tp": 89,
      "fp": 138,
      "tn": 252,
      "fn": 135,
      "n": 614
    },
    "cat::self-harm": {
      "acc": 0.7987421383647799,
      "prec": 0.47368421052631576,
      "rec": 0.23076923076923078,
      "f1": 0.3103448275862069,
      "tp": 36,
      "fp": 40,
      "tn": 599,
      "fn": 120,
      "n": 795
    },
    "cat::sexual": {
      "acc": 0.6144721233689205,
      "prec": 0.6977186311787072,
      "rec": 0.6885553470919324,
      "f1": 0.6931067044381491,
      "tp": 367,
      "fp": 159,
      "tn": 151,
      "fn": 166,
      "n": 843
    },
    "cat::shocking": {
      "acc": 0.5304347826086957,
      "prec": 0.5512820512820513,
      "rec": 0.5375,
      "f1": 0.5443037974683544,
      "tp": 258,
      "fp": 210,
      "tn": 230,
      "fn": 222,
      "n": 920
    },
    "cat::spam": {
      "acc": 0.6509433962264151,
      "prec": 0.6009615384615384,
      "rec": 0.4734848484848485,
      "f1": 0.5296610169491526,
      "tp": 125,
      "fp": 83,
      "tn": 289,
      "fn": 139,
      "n": 636
    },
    "cat::violence": {
      "acc": 0.6245110821382008,
      "prec": 0.5592705167173252,
      "rec": 0.5626911314984709,
      "f1": 0.5609756097560976,
      "tp": 184,
      "fp": 145,
      "tn": 295,
      "fn": 143,
      "n": 767
    },
    "src::Laion5B": {
      "acc": 0.6561186650185414,
      "prec": 0.5239220318960425,
      "rec": 0.6025815217391305,
      "f1": 0.5605055292259085,
      "tp": 887,
      "fp": 806,
      "tn": 1767,
      "fn": 585,
      "n": 4045
    },
    "src::Lexica": {
      "acc": 0.640994094488189,
      "prec": 0.6190476190476191,
      "rec": 0.4913841022790439,
      "f1": 0.5478772854044004,
      "tp": 884,
      "fp": 544,
      "tn": 1721,
      "fn": 915,
      "n": 4064
    },
    "overall_probs": {
      "pr_auc": 0.5324942696776896,
      "roc_auc": 0.6659141068194333,
      "ece_15": 0.1889033429789825,
      "recall_at_1pct_fpr": 0.010700091715071844,
      "recall_at_5pct_fpr": 0.10944665239987772,
      "fpr_at_90pct_recall": 0.7331541959487391,
      "fpr_at_95pct_recall": 0.8412567176519223
    },
    "cat_probs::deception": {
      "pr_auc": 0.5690349828940175,
      "roc_auc": 0.6612945287644083,
      "ece_15": 0.14537941146391117
    },
    "cat_probs::harassment": {
      "pr_auc": 0.47909668058116517,
      "roc_auc": 0.7336355152646314,
      "ece_15": 0.25985474356630534
    },
    "cat_probs::hate": {
      "pr_auc": 0.1847995806257408,
      "roc_auc": 0.5181124219292158,
      "ece_15": 0.1604414309386615
    },
    "cat_probs::illegal activity": {
      "pr_auc": 0.5709491509759578,
      "roc_auc": 0.5670659191301393,
      "ece_15": 0.24551940888872714
    },
    "cat_probs::political": {
      "pr_auc": 0.710779930394732,
      "roc_auc": 0.7249830393487109,
      "ece_15": 0.14202469047502594
    },
    "cat_probs::public and personal health": {
      "pr_auc": 0.3736944186159322,
      "roc_auc": 0.5342376373626374,
      "ece_15": 0.317428194461178
    },
    "cat_probs::self-harm": {
      "pr_auc": 0.4011410904200405,
      "roc_auc": 0.7251815737731231,
      "ece_15": 0.11776717338731525
    },
    "cat_probs::sexual": {
      "pr_auc": 0.7355346680200631,
      "roc_auc": 0.6242449918295709,
      "ece_15": 0.21927709823467662
    },
    "cat_probs::shocking": {
      "pr_auc": 0.5093078048943659,
      "roc_auc": 0.516065340909091,
      "ece_15": 0.2908984359170301
    },
    "cat_probs::spam": {
      "pr_auc": 0.5202932814357553,
      "roc_auc": 0.6649661942000651,
      "ece_15": 0.29730834610223983
    },
    "cat_probs::violence": {
      "pr_auc": 0.572476740838473,
      "roc_auc": 0.6816722268557132,
      "ece_15": 0.2063909554379343
    },
    "src_probs::Laion5B": {
      "pr_auc": 0.4918957327286272,
      "roc_auc": 0.6795025473563258,
      "ece_15": 0.20282063496259373
    },
    "src_probs::Lexica": {
      "pr_auc": 0.6081525268062793,
      "roc_auc": 0.6603818407822841,
      "ece_15": 0.1815469314641503
    }
  },
  "meta": {
    "duration_sec": 469.35236930000247,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 4240799232,
    "gpu_peak_reserved_bytes": 5114953728
  }
}