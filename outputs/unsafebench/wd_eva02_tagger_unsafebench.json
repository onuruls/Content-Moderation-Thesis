{
  "thr": 0.15434611559590428,
  "thr_by_cat": {
    "hate": 0.15104063783975985,
    "violence": 0.17009407849168845,
    "self-harm": 0.432272626768868,
    "sexual": 0.5292244726794191,
    "shocking": 0.1656069548233696,
    "illegal activity": 0.1,
    "deception": 0.05,
    "political": 0.05,
    "public and personal health": 0.1,
    "spam": 0.05,
    "harassment": 0.4125289303367963
  },
  "results": {
    "overall": {
      "acc": 0.6529209621993127,
      "prec": 0.5455729166666666,
      "rec": 0.5392535392535392,
      "f1": 0.5423948220064725,
      "tp": 419,
      "fp": 349,
      "tn": 911,
      "fn": 358,
      "n": 2037
    },
    "cat::deception": {
      "acc": 0.5828220858895705,
      "prec": 0.2857142857142857,
      "rec": 0.24,
      "f1": 0.2608695652173913,
      "tp": 12,
      "fp": 30,
      "tn": 83,
      "fn": 38,
      "n": 163
    },
    "cat::harassment": {
      "acc": 0.6417112299465241,
      "prec": 0.24528301886792453,
      "rec": 0.325,
      "f1": 0.27956989247311825,
      "tp": 13,
      "fp": 40,
      "tn": 107,
      "fn": 27,
      "n": 187
    },
    "cat::hate": {
      "acc": 0.7839195979899497,
      "prec": 0.23076923076923078,
      "rec": 0.20689655172413793,
      "f1": 0.21818181818181817,
      "tp": 6,
      "fp": 20,
      "tn": 150,
      "fn": 23,
      "n": 199
    },
    "cat::illegal activity": {
      "acc": 0.5872093023255814,
      "prec": 0.6268656716417911,
      "rec": 0.4772727272727273,
      "f1": 0.5419354838709678,
      "tp": 42,
      "fp": 25,
      "tn": 59,
      "fn": 46,
      "n": 172
    },
    "cat::political": {
      "acc": 0.6107784431137725,
      "prec": 0.6477272727272727,
      "rec": 0.6263736263736264,
      "f1": 0.6368715083798883,
      "tp": 57,
      "fp": 31,
      "tn": 45,
      "fn": 34,
      "n": 167
    },
    "cat::public and personal health": {
      "acc": 0.6,
      "prec": 0.4507042253521127,
      "rec": 0.5818181818181818,
      "f1": 0.5079365079365079,
      "tp": 32,
      "fp": 39,
      "tn": 61,
      "fn": 23,
      "n": 155
    },
    "cat::self-harm": {
      "acc": 0.7336683417085427,
      "prec": 0.125,
      "rec": 0.13793103448275862,
      "f1": 0.13114754098360656,
      "tp": 4,
      "fp": 28,
      "tn": 142,
      "fn": 25,
      "n": 199
    },
    "cat::sexual": {
      "acc": 0.6445497630331753,
      "prec": 0.7777777777777778,
      "rec": 0.7,
      "f1": 0.7368421052631577,
      "tp": 105,
      "fp": 30,
      "tn": 31,
      "fn": 45,
      "n": 211
    },
    "cat::shocking": {
      "acc": 0.6536796536796536,
      "prec": 0.6349206349206349,
      "rec": 0.7017543859649122,
      "f1": 0.6666666666666666,
      "tp": 80,
      "fp": 46,
      "tn": 71,
      "fn": 34,
      "n": 231
    },
    "cat::spam": {
      "acc": 0.625,
      "prec": 0.4166666666666667,
      "rec": 0.38461538461538464,
      "f1": 0.4,
      "tp": 20,
      "fp": 28,
      "tn": 80,
      "fn": 32,
      "n": 160
    },
    "cat::violence": {
      "acc": 0.6735751295336787,
      "prec": 0.6,
      "rec": 0.6075949367088608,
      "f1": 0.6037735849056604,
      "tp": 48,
      "fp": 32,
      "tn": 82,
      "fn": 31,
      "n": 193
    },
    "src::Laion5B": {
      "acc": 0.6315270935960591,
      "prec": 0.4782608695652174,
      "rec": 0.42777777777777776,
      "f1": 0.4516129032258065,
      "tp": 154,
      "fp": 168,
      "tn": 487,
      "fn": 206,
      "n": 1015
    },
    "src::Lexica": {
      "acc": 0.6741682974559687,
      "prec": 0.594170403587444,
      "rec": 0.6354916067146283,
      "f1": 0.6141367323290846,
      "tp": 265,
      "fp": 181,
      "tn": 424,
      "fn": 152,
      "n": 1022
    }
  },
  "meta": {
    "duration_sec": 581.0265931000004,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 3155956224,
    "gpu_peak_reserved_bytes": 6249512960
  }
}