{
  "thr": 0.6369357290919486,
  "thr_by_cat": {
    "hate": 0.6371267618783356,
    "violence": 0.6508040281287073,
    "self-harm": 0.7016682001779665,
    "sexual": 0.6970302652775322,
    "shocking": 0.6438799785531085,
    "illegal activity": 0.5887615771793382,
    "deception": 0.5884623698221272,
    "political": 0.5731490064098175,
    "public and personal health": 0.6557522594734589,
    "spam": 0.58148196958146,
    "harassment": 0.70631130507101
  },
  "results": {
    "overall": {
      "acc": 0.6440844378988709,
      "prec": 0.5331632653061225,
      "rec": 0.5379665379665379,
      "f1": 0.535554131966688,
      "tp": 418,
      "fp": 366,
      "tn": 894,
      "fn": 359,
      "n": 2037
    },
    "cat::deception": {
      "acc": 0.5398773006134969,
      "prec": 0.2807017543859649,
      "rec": 0.32,
      "f1": 0.29906542056074764,
      "tp": 16,
      "fp": 41,
      "tn": 72,
      "fn": 34,
      "n": 163
    },
    "cat::harassment": {
      "acc": 0.6524064171122995,
      "prec": 0.2222222222222222,
      "rec": 0.25,
      "f1": 0.23529411764705882,
      "tp": 10,
      "fp": 35,
      "tn": 112,
      "fn": 30,
      "n": 187
    },
    "cat::hate": {
      "acc": 0.7537688442211056,
      "prec": 0.11538461538461539,
      "rec": 0.10344827586206896,
      "f1": 0.10909090909090909,
      "tp": 3,
      "fp": 23,
      "tn": 147,
      "fn": 26,
      "n": 199
    },
    "cat::illegal activity": {
      "acc": 0.5116279069767442,
      "prec": 0.5238095238095238,
      "rec": 0.5,
      "f1": 0.5116279069767442,
      "tp": 44,
      "fp": 40,
      "tn": 44,
      "fn": 44,
      "n": 172
    },
    "cat::political": {
      "acc": 0.5808383233532934,
      "prec": 0.6129032258064516,
      "rec": 0.6263736263736264,
      "f1": 0.6195652173913044,
      "tp": 57,
      "fp": 36,
      "tn": 40,
      "fn": 34,
      "n": 167
    },
    "cat::public and personal health": {
      "acc": 0.6516129032258065,
      "prec": 0.5084745762711864,
      "rec": 0.5454545454545454,
      "f1": 0.5263157894736842,
      "tp": 30,
      "fp": 29,
      "tn": 71,
      "fn": 25,
      "n": 155
    },
    "cat::self-harm": {
      "acc": 0.7286432160804021,
      "prec": 0.14285714285714285,
      "rec": 0.1724137931034483,
      "f1": 0.15625,
      "tp": 5,
      "fp": 30,
      "tn": 140,
      "fn": 24,
      "n": 199
    },
    "cat::sexual": {
      "acc": 0.6445497630331753,
      "prec": 0.7622377622377622,
      "rec": 0.7266666666666667,
      "f1": 0.7440273037542663,
      "tp": 109,
      "fp": 34,
      "tn": 27,
      "fn": 41,
      "n": 211
    },
    "cat::shocking": {
      "acc": 0.670995670995671,
      "prec": 0.6666666666666666,
      "rec": 0.6666666666666666,
      "f1": 0.6666666666666666,
      "tp": 76,
      "fp": 38,
      "tn": 79,
      "fn": 38,
      "n": 231
    },
    "cat::spam": {
      "acc": 0.63125,
      "prec": 0.43636363636363634,
      "rec": 0.46153846153846156,
      "f1": 0.4485981308411215,
      "tp": 24,
      "fp": 31,
      "tn": 77,
      "fn": 28,
      "n": 160
    },
    "cat::violence": {
      "acc": 0.6683937823834197,
      "prec": 0.6027397260273972,
      "rec": 0.5569620253164557,
      "f1": 0.5789473684210525,
      "tp": 44,
      "fp": 29,
      "tn": 85,
      "fn": 35,
      "n": 193
    },
    "src::Laion5B": {
      "acc": 0.625615763546798,
      "prec": 0.46875,
      "rec": 0.4166666666666667,
      "f1": 0.4411764705882353,
      "tp": 150,
      "fp": 170,
      "tn": 485,
      "fn": 210,
      "n": 1015
    },
    "src::Lexica": {
      "acc": 0.662426614481409,
      "prec": 0.5775862068965517,
      "rec": 0.6426858513189448,
      "f1": 0.6083995459704881,
      "tp": 268,
      "fp": 196,
      "tn": 409,
      "fn": 149,
      "n": 1022
    },
    "overall_probs": {
      "pr_auc": 0.5135025111157806,
      "roc_auc": 0.6305422769708484,
      "ece_15": 0.24017742922859678,
      "recall_at_1pct_fpr": 0.036036036036036036,
      "recall_at_5pct_fpr": 0.1402831402831403,
      "fpr_at_90pct_recall": 0.8158730158730159,
      "fpr_at_95pct_recall": 0.9007936507936508
    },
    "cat_probs::deception": {
      "pr_auc": 0.29728319089946154,
      "roc_auc": 0.4902654867256637,
      "ece_15": 0.2508776189725092
    },
    "cat_probs::harassment": {
      "pr_auc": 0.2857237166499814,
      "roc_auc": 0.6333333333333333,
      "ece_15": 0.4315667823355466
    },
    "cat_probs::hate": {
      "pr_auc": 0.14021151209741445,
      "roc_auc": 0.46572008113590263,
      "ece_15": 0.41508845378406084
    },
    "cat_probs::illegal activity": {
      "pr_auc": 0.49316499248413653,
      "roc_auc": 0.5175865800865801,
      "ece_15": 0.11913911790348762
    },
    "cat_probs::political": {
      "pr_auc": 0.6065709720878343,
      "roc_auc": 0.5989010989010989,
      "ece_15": 0.05321867291085022
    },
    "cat_probs::public and personal health": {
      "pr_auc": 0.508045997744517,
      "roc_auc": 0.676,
      "ece_15": 0.27511881224570733
    },
    "cat_probs::self-harm": {
      "pr_auc": 0.18876163885755626,
      "roc_auc": 0.4955375253549696,
      "ece_15": 0.47470164883076843
    },
    "cat_probs::sexual": {
      "pr_auc": 0.8111192845331254,
      "roc_auc": 0.6375956284153006,
      "ece_15": 0.08880720644200582
    },
    "cat_probs::shocking": {
      "pr_auc": 0.6153828047066708,
      "roc_auc": 0.6539961013645224,
      "ece_15": 0.1709658387419465
    },
    "cat_probs::spam": {
      "pr_auc": 0.4026142471210065,
      "roc_auc": 0.5966880341880342,
      "ece_15": 0.24160679168999194
    },
    "cat_probs::violence": {
      "pr_auc": 0.5360142811027504,
      "roc_auc": 0.6596713302242949,
      "ece_15": 0.22173507210504204
    },
    "src_probs::Laion5B": {
      "pr_auc": 0.4369691536388236,
      "roc_auc": 0.5677735368956742,
      "ece_15": 0.25414497737814057
    },
    "src_probs::Lexica": {
      "pr_auc": 0.5760572567327278,
      "roc_auc": 0.6777315337812395,
      "ece_15": 0.2263055492170636
    }
  },
  "meta": {
    "duration_sec": 618.212840799999,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 0,
    "gpu_peak_reserved_bytes": 0
  }
}