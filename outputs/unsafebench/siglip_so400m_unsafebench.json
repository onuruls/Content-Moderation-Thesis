{
  "thr": 0.9358226995989138,
  "thr_by_cat": {
    "hate": 0.9328930377960205,
    "violence": 0.8694374462616336,
    "self-harm": 0.95,
    "sexual": 0.8958098727895979,
    "shocking": 0.8660595131957013,
    "illegal activity": 0.8535872114797867,
    "deception": 0.9104752627319596,
    "political": 0.95,
    "public and personal health": 0.95,
    "spam": 0.9105158920557994,
    "harassment": 0.95
  },
  "results": {
    "overall": {
      "acc": 0.665193912616593,
      "prec": 0.5519125683060109,
      "rec": 0.6499356499356499,
      "f1": 0.5969267139479905,
      "tp": 505,
      "fp": 410,
      "tn": 850,
      "fn": 272,
      "n": 2037
    },
    "cat::deception": {
      "acc": 0.6871165644171779,
      "prec": 0.49056603773584906,
      "rec": 0.52,
      "f1": 0.5048543689320388,
      "tp": 26,
      "fp": 27,
      "tn": 86,
      "fn": 24,
      "n": 163
    },
    "cat::harassment": {
      "acc": 0.6203208556149733,
      "prec": 0.3258426966292135,
      "rec": 0.725,
      "f1": 0.4496124031007752,
      "tp": 29,
      "fp": 60,
      "tn": 87,
      "fn": 11,
      "n": 187
    },
    "cat::hate": {
      "acc": 0.7336683417085427,
      "prec": 0.26,
      "rec": 0.4482758620689655,
      "f1": 0.3291139240506329,
      "tp": 13,
      "fp": 37,
      "tn": 133,
      "fn": 16,
      "n": 199
    },
    "cat::illegal activity": {
      "acc": 0.4883720930232558,
      "prec": 0.5,
      "rec": 0.4431818181818182,
      "f1": 0.46987951807228917,
      "tp": 39,
      "fp": 39,
      "tn": 45,
      "fn": 49,
      "n": 172
    },
    "cat::political": {
      "acc": 0.7365269461077845,
      "prec": 0.7155963302752294,
      "rec": 0.8571428571428571,
      "f1": 0.7799999999999999,
      "tp": 78,
      "fp": 31,
      "tn": 45,
      "fn": 13,
      "n": 167
    },
    "cat::public and personal health": {
      "acc": 0.5161290322580645,
      "prec": 0.3484848484848485,
      "rec": 0.41818181818181815,
      "f1": 0.3801652892561984,
      "tp": 23,
      "fp": 43,
      "tn": 57,
      "fn": 32,
      "n": 155
    },
    "cat::self-harm": {
      "acc": 0.7135678391959799,
      "prec": 0.27419354838709675,
      "rec": 0.5862068965517241,
      "f1": 0.3736263736263736,
      "tp": 17,
      "fp": 45,
      "tn": 125,
      "fn": 12,
      "n": 199
    },
    "cat::sexual": {
      "acc": 0.7535545023696683,
      "prec": 0.8141025641025641,
      "rec": 0.8466666666666667,
      "f1": 0.8300653594771242,
      "tp": 127,
      "fp": 29,
      "tn": 32,
      "fn": 23,
      "n": 211
    },
    "cat::shocking": {
      "acc": 0.6536796536796536,
      "prec": 0.6440677966101694,
      "rec": 0.6666666666666666,
      "f1": 0.6551724137931034,
      "tp": 76,
      "fp": 42,
      "tn": 75,
      "fn": 38,
      "n": 231
    },
    "cat::spam": {
      "acc": 0.68125,
      "prec": 0.5094339622641509,
      "rec": 0.5192307692307693,
      "f1": 0.5142857142857143,
      "tp": 27,
      "fp": 26,
      "tn": 82,
      "fn": 25,
      "n": 160
    },
    "cat::violence": {
      "acc": 0.689119170984456,
      "prec": 0.6172839506172839,
      "rec": 0.6329113924050633,
      "f1": 0.6250000000000001,
      "tp": 50,
      "fp": 31,
      "tn": 83,
      "fn": 29,
      "n": 193
    },
    "src::Laion5B": {
      "acc": 0.6551724137931034,
      "prec": 0.5100401606425703,
      "rec": 0.7055555555555556,
      "f1": 0.5920745920745921,
      "tp": 254,
      "fp": 244,
      "tn": 411,
      "fn": 106,
      "n": 1015
    },
    "src::Lexica": {
      "acc": 0.675146771037182,
      "prec": 0.6019184652278178,
      "rec": 0.6019184652278178,
      "f1": 0.6019184652278178,
      "tp": 251,
      "fp": 166,
      "tn": 439,
      "fn": 166,
      "n": 1022
    }
  },
  "meta": {
    "duration_sec": 419.34455990000015,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 3724899328,
    "gpu_peak_reserved_bytes": 4886364160
  }
}