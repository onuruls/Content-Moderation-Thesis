{
  "thr": 0.6165211065528319,
  "thr_by_cat": {
    "public and personal health": 0.46604104484281234,
    "spam": 0.5670877650380135,
    "hate": 0.7396570486039972,
    "harassment": 0.6040829193783317,
    "violence": 0.5396573015445255,
    "self-harm": 0.8990062595012799,
    "sexual": 0.5080816031067292,
    "shocking": 0.5735832630813896,
    "illegal activity": 0.35640255171199176,
    "deception": 0.618768387045597,
    "political": 0.6843010419143175
  },
  "results": {
    "overall": {
      "acc": 0.6516216549512887,
      "prec": 0.5702141057934509,
      "rec": 0.5536533170284317,
      "f1": 0.561811695362184,
      "tp": 1811,
      "fp": 1365,
      "tn": 3473,
      "fn": 1460,
      "n": 8109
    },
    "cat::deception": {
      "acc": 0.6362229102167183,
      "prec": 0.48936170212765956,
      "rec": 0.39826839826839827,
      "f1": 0.4391408114558472,
      "tp": 92,
      "fp": 96,
      "tn": 319,
      "fn": 139,
      "n": 646
    },
    "cat::harassment": {
      "acc": 0.7238605898123325,
      "prec": 0.36496350364963503,
      "rec": 0.2958579881656805,
      "f1": 0.3267973856209151,
      "tp": 50,
      "fp": 87,
      "tn": 490,
      "fn": 119,
      "n": 746
    },
    "cat::hate": {
      "acc": 0.706700379266751,
      "prec": 0.17834394904458598,
      "rec": 0.21374045801526717,
      "f1": 0.19444444444444445,
      "tp": 28,
      "fp": 129,
      "tn": 531,
      "fn": 103,
      "n": 791
    },
    "cat::illegal activity": {
      "acc": 0.62882096069869,
      "prec": 0.6438356164383562,
      "rec": 0.6527777777777778,
      "f1": 0.6482758620689656,
      "tp": 235,
      "fp": 130,
      "tn": 197,
      "fn": 125,
      "n": 687
    },
    "cat::political": {
      "acc": 0.7048192771084337,
      "prec": 0.7604166666666666,
      "rec": 0.7373737373737373,
      "f1": 0.7487179487179487,
      "tp": 292,
      "fp": 92,
      "tn": 176,
      "fn": 104,
      "n": 664
    },
    "cat::public and personal health": {
      "acc": 0.5553745928338762,
      "prec": 0.40239043824701193,
      "rec": 0.45089285714285715,
      "f1": 0.4252631578947369,
      "tp": 101,
      "fp": 150,
      "tn": 240,
      "fn": 123,
      "n": 614
    },
    "cat::self-harm": {
      "acc": 0.7144654088050314,
      "prec": 0.20168067226890757,
      "rec": 0.15384615384615385,
      "f1": 0.17454545454545453,
      "tp": 24,
      "fp": 95,
      "tn": 544,
      "fn": 132,
      "n": 795
    },
    "cat::sexual": {
      "acc": 0.6346381969157769,
      "prec": 0.6910016977928692,
      "rec": 0.7636022514071295,
      "f1": 0.7254901960784315,
      "tp": 407,
      "fp": 182,
      "tn": 128,
      "fn": 126,
      "n": 843
    },
    "cat::shocking": {
      "acc": 0.6141304347826086,
      "prec": 0.6535626535626535,
      "rec": 0.5541666666666667,
      "f1": 0.5997745208568207,
      "tp": 266,
      "fp": 141,
      "tn": 299,
      "fn": 214,
      "n": 920
    },
    "cat::spam": {
      "acc": 0.5408805031446541,
      "prec": 0.44017094017094016,
      "rec": 0.39015151515151514,
      "f1": 0.41365461847389556,
      "tp": 103,
      "fp": 131,
      "tn": 241,
      "fn": 161,
      "n": 636
    },
    "cat::violence": {
      "acc": 0.6792698826597132,
      "prec": 0.6173913043478261,
      "rec": 0.6513761467889908,
      "f1": 0.6339285714285715,
      "tp": 213,
      "fp": 132,
      "tn": 308,
      "fn": 114,
      "n": 767
    },
    "src::Laion5B": {
      "acc": 0.6529048207663782,
      "prec": 0.5248538011695907,
      "rec": 0.48777173913043476,
      "f1": 0.5056338028169014,
      "tp": 718,
      "fp": 650,
      "tn": 1923,
      "fn": 754,
      "n": 4045
    },
    "src::Lexica": {
      "acc": 0.6503444881889764,
      "prec": 0.6045353982300885,
      "rec": 0.6075597554196775,
      "f1": 0.6060438037149987,
      "tp": 1093,
      "fp": 715,
      "tn": 1550,
      "fn": 706,
      "n": 4064
    },
    "overall_probs": {
      "pr_auc": 0.5042880035355783,
      "roc_auc": 0.6277754488471414,
      "ece_15": 0.1515362899557982,
      "recall_at_1pct_fpr": 0.009477224090492203,
      "recall_at_5pct_fpr": 0.10455518190155916,
      "fpr_at_90pct_recall": 0.7788342290202563,
      "fpr_at_95pct_recall": 0.8637866887143447
    },
    "cat_probs::deception": {
      "pr_auc": 0.5232045714966489,
      "roc_auc": 0.6477442236478381,
      "ece_15": 0.15340636585242237
    },
    "cat_probs::harassment": {
      "pr_auc": 0.36916442322775445,
      "roc_auc": 0.6797247546480982,
      "ece_15": 0.1736365688350501
    },
    "cat_probs::hate": {
      "pr_auc": 0.17398339505626448,
      "roc_auc": 0.4955355077492482,
      "ece_15": 0.426188575407648
    },
    "cat_probs::illegal activity": {
      "pr_auc": 0.6552851722591464,
      "roc_auc": 0.6643433571185864,
      "ece_15": 0.15887662737184297
    },
    "cat_probs::political": {
      "pr_auc": 0.7914244720368018,
      "roc_auc": 0.7585180159807026,
      "ece_15": 0.09538823525499866
    },
    "cat_probs::public and personal health": {
      "pr_auc": 0.41142731797688153,
      "roc_auc": 0.5763736263736264,
      "ece_15": 0.1623316566044228
    },
    "cat_probs::self-harm": {
      "pr_auc": 0.20012808315956637,
      "roc_auc": 0.49014385458047427,
      "ece_15": 0.4498591153595432
    },
    "cat_probs::sexual": {
      "pr_auc": 0.7662056822097176,
      "roc_auc": 0.6550021182593959,
      "ece_15": 0.08521918988259662
    },
    "cat_probs::shocking": {
      "pr_auc": 0.6575547628056941,
      "roc_auc": 0.660686553030303,
      "ece_15": 0.07137858387406754
    },
    "cat_probs::spam": {
      "pr_auc": 0.4196994906371154,
      "roc_auc": 0.5587172124470512,
      "ece_15": 0.20055384878889196
    },
    "cat_probs::violence": {
      "pr_auc": 0.6726365961037453,
      "roc_auc": 0.732068390325271,
      "ece_15": 0.0913848061056266
    },
    "src_probs::Laion5B": {
      "pr_auc": 0.43089643671306754,
      "roc_auc": 0.6200016581050711,
      "ece_15": 0.14885186467121717
    },
    "src_probs::Lexica": {
      "pr_auc": 0.5867078084537811,
      "roc_auc": 0.6334680660214713,
      "ece_15": 0.15787058762379183
    }
  },
  "meta": {
    "duration_sec": 394.23882100000264,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 3129551360,
    "gpu_peak_reserved_bytes": 4192206848
  }
}