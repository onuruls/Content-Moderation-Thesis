{
  "thr": 0.6926419734954834,
  "thr_by_cat": {
    "public and personal health": 0.7310585379600525,
    "spam": 0.6791787147521973,
    "hate": 0.7431679368019104,
    "harassment": 0.7212605039703656,
    "violence": 0.7057850360870361,
    "self-harm": 0.7310585379600525,
    "sexual": 0.632818229673033,
    "shocking": 0.665410578250885,
    "illegal activity": 0.7185943722724915,
    "deception": 0.6926419734954834,
    "political": 0.6926419734954834
  },
  "results": {
    "overall": {
      "acc": 0.5887285731902824,
      "prec": 0.4899874843554443,
      "rec": 0.4787526750229288,
      "f1": 0.484304932735426,
      "tp": 1566,
      "fp": 1630,
      "tn": 3208,
      "fn": 1705,
      "n": 8109
    },
    "cat::deception": {
      "acc": 0.5928792569659442,
      "prec": 0.4238095238095238,
      "rec": 0.3852813852813853,
      "f1": 0.4036281179138322,
      "tp": 89,
      "fp": 121,
      "tn": 294,
      "fn": 142,
      "n": 646
    },
    "cat::harassment": {
      "acc": 0.646112600536193,
      "prec": 0.263681592039801,
      "rec": 0.3136094674556213,
      "f1": 0.2864864864864865,
      "tp": 53,
      "fp": 148,
      "tn": 429,
      "fn": 116,
      "n": 746
    },
    "cat::hate": {
      "acc": 0.7281921618204804,
      "prec": 0.1557377049180328,
      "rec": 0.1450381679389313,
      "f1": 0.15019762845849804,
      "tp": 19,
      "fp": 103,
      "tn": 557,
      "fn": 112,
      "n": 791
    },
    "cat::illegal activity": {
      "acc": 0.4264919941775837,
      "prec": 0.45478723404255317,
      "rec": 0.475,
      "f1": 0.4646739130434782,
      "tp": 171,
      "fp": 205,
      "tn": 122,
      "fn": 189,
      "n": 687
    },
    "cat::political": {
      "acc": 0.6987951807228916,
      "prec": 0.7207207207207207,
      "rec": 0.8080808080808081,
      "f1": 0.7619047619047618,
      "tp": 320,
      "fp": 124,
      "tn": 144,
      "fn": 76,
      "n": 664
    },
    "cat::public and personal health": {
      "acc": 0.501628664495114,
      "prec": 0.3217391304347826,
      "rec": 0.33035714285714285,
      "f1": 0.32599118942731276,
      "tp": 74,
      "fp": 156,
      "tn": 234,
      "fn": 150,
      "n": 614
    },
    "cat::self-harm": {
      "acc": 0.6943396226415094,
      "prec": 0.14634146341463414,
      "rec": 0.11538461538461539,
      "f1": 0.12903225806451613,
      "tp": 18,
      "fp": 105,
      "tn": 534,
      "fn": 138,
      "n": 795
    },
    "cat::sexual": {
      "acc": 0.5693950177935944,
      "prec": 0.6609848484848485,
      "rec": 0.6547842401500938,
      "f1": 0.6578699340245052,
      "tp": 349,
      "fp": 179,
      "tn": 131,
      "fn": 184,
      "n": 843
    },
    "cat::shocking": {
      "acc": 0.4597826086956522,
      "prec": 0.47858942065491183,
      "rec": 0.3958333333333333,
      "f1": 0.4332953249714937,
      "tp": 190,
      "fp": 207,
      "tn": 233,
      "fn": 290,
      "n": 920
    },
    "cat::spam": {
      "acc": 0.6242138364779874,
      "prec": 0.5550660792951542,
      "rec": 0.4772727272727273,
      "f1": 0.5132382892057026,
      "tp": 126,
      "fp": 101,
      "tn": 271,
      "fn": 138,
      "n": 636
    },
    "cat::violence": {
      "acc": 0.5423728813559322,
      "prec": 0.46449704142011833,
      "rec": 0.4801223241590214,
      "f1": 0.47218045112781953,
      "tp": 157,
      "fp": 181,
      "tn": 259,
      "fn": 170,
      "n": 767
    },
    "src::Laion5B": {
      "acc": 0.5841779975278121,
      "prec": 0.4256373937677054,
      "rec": 0.40828804347826086,
      "f1": 0.4167822468793343,
      "tp": 601,
      "fp": 811,
      "tn": 1762,
      "fn": 871,
      "n": 4045
    },
    "src::Lexica": {
      "acc": 0.593257874015748,
      "prec": 0.5409192825112108,
      "rec": 0.5364091161756531,
      "f1": 0.5386547585821936,
      "tp": 965,
      "fp": 819,
      "tn": 1446,
      "fn": 834,
      "n": 4064
    },
    "overall_probs": {
      "pr_auc": 0.4562597833968517,
      "roc_auc": 0.5609141567401352,
      "ece_15": 0.2590109363725674,
      "recall_at_1pct_fpr": 0.020483032711708957,
      "recall_at_5pct_fpr": 0.07092632222561908,
      "fpr_at_90pct_recall": 0.8594460520876395,
      "fpr_at_95pct_recall": 0.9338569656883009
    },
    "cat_probs::deception": {
      "pr_auc": 0.4778361249979299,
      "roc_auc": 0.553533614979398,
      "ece_15": 0.2866381859354928
    },
    "cat_probs::harassment": {
      "pr_auc": 0.24284923068146444,
      "roc_auc": 0.5563514608308635,
      "ece_15": 0.4390488859395878
    },
    "cat_probs::hate": {
      "pr_auc": 0.1547306086490211,
      "roc_auc": 0.4613173721952348,
      "ece_15": 0.47834903838209497
    },
    "cat_probs::illegal activity": {
      "pr_auc": 0.4463588274781295,
      "roc_auc": 0.37259599048589875,
      "ece_15": 0.22005869623875512
    },
    "cat_probs::political": {
      "pr_auc": 0.7635571550478348,
      "roc_auc": 0.7309145560078396,
      "ece_15": 0.12926055078046866
    },
    "cat_probs::public and personal health": {
      "pr_auc": 0.33824713410269847,
      "roc_auc": 0.4791952838827839,
      "ece_15": 0.33316422202882234
    },
    "cat_probs::self-harm": {
      "pr_auc": 0.19528320416944145,
      "roc_auc": 0.49623309658520925,
      "ece_15": 0.41980619430542
    },
    "cat_probs::sexual": {
      "pr_auc": 0.6764018258548665,
      "roc_auc": 0.5641136597470192,
      "ece_15": 0.04761055903785455
    },
    "cat_probs::shocking": {
      "pr_auc": 0.48278750285904454,
      "roc_auc": 0.45249526515151517,
      "ece_15": 0.155310482531786
    },
    "cat_probs::spam": {
      "pr_auc": 0.5233095093321175,
      "roc_auc": 0.6565860215053765,
      "ece_15": 0.2107125810294781
    },
    "cat_probs::violence": {
      "pr_auc": 0.46185436115300044,
      "roc_auc": 0.5433625243258271,
      "ece_15": 0.25525962972267974
    },
    "src_probs::Laion5B": {
      "pr_auc": 0.3900941688499912,
      "roc_auc": 0.5394658050152925,
      "ece_15": 0.2869129266741838
    },
    "src_probs::Lexica": {
      "pr_auc": 0.5205071489907293,
      "roc_auc": 0.5747511187844119,
      "ece_15": 0.23150724724052457
    }
  },
  "meta": {
    "duration_sec": 10976.492653600002,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 12039081984,
    "gpu_peak_reserved_bytes": 13658750976
  }
}