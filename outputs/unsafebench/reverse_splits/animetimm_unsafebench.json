{
  "thr": 0.28363351723582475,
  "thr_by_cat": {
    "public and personal health": 0.4939043598790323,
    "spam": 0.07316131591796876,
    "hate": 0.23041722283291455,
    "harassment": 0.5224609375,
    "violence": 0.23584047623866583,
    "self-harm": 0.7656004632537686,
    "sexual": 0.485595703125,
    "shocking": 0.39060914671266234,
    "illegal activity": 0.06380728788154069,
    "deception": 0.05,
    "political": 0.05
  },
  "results": {
    "overall": {
      "acc": 0.6380564804538168,
      "prec": 0.5506329113924051,
      "rec": 0.5585447875267502,
      "f1": 0.5545606313552891,
      "tp": 1827,
      "fp": 1491,
      "tn": 3347,
      "fn": 1444,
      "n": 8109
    },
    "cat::deception": {
      "acc": 0.5510835913312694,
      "prec": 0.36018957345971564,
      "rec": 0.329004329004329,
      "f1": 0.3438914027149321,
      "tp": 76,
      "fp": 135,
      "tn": 280,
      "fn": 155,
      "n": 646
    },
    "cat::harassment": {
      "acc": 0.6863270777479893,
      "prec": 0.35555555555555557,
      "rec": 0.47337278106508873,
      "f1": 0.40609137055837563,
      "tp": 80,
      "fp": 145,
      "tn": 432,
      "fn": 89,
      "n": 746
    },
    "cat::hate": {
      "acc": 0.7420986093552465,
      "prec": 0.21705426356589147,
      "rec": 0.21374045801526717,
      "f1": 0.2153846153846154,
      "tp": 28,
      "fp": 101,
      "tn": 559,
      "fn": 103,
      "n": 791
    },
    "cat::illegal activity": {
      "acc": 0.49199417758369723,
      "prec": 0.5157593123209169,
      "rec": 0.5,
      "f1": 0.5077574047954866,
      "tp": 180,
      "fp": 169,
      "tn": 158,
      "fn": 180,
      "n": 687
    },
    "cat::political": {
      "acc": 0.5783132530120482,
      "prec": 0.6676300578034682,
      "rec": 0.5833333333333334,
      "f1": 0.6226415094339622,
      "tp": 231,
      "fp": 115,
      "tn": 153,
      "fn": 165,
      "n": 664
    },
    "cat::public and personal health": {
      "acc": 0.5895765472312704,
      "prec": 0.4326923076923077,
      "rec": 0.4017857142857143,
      "f1": 0.4166666666666667,
      "tp": 90,
      "fp": 118,
      "tn": 272,
      "fn": 134,
      "n": 614
    },
    "cat::self-harm": {
      "acc": 0.7547169811320755,
      "prec": 0.3617021276595745,
      "rec": 0.3269230769230769,
      "f1": 0.3434343434343434,
      "tp": 51,
      "fp": 90,
      "tn": 549,
      "fn": 105,
      "n": 795
    },
    "cat::sexual": {
      "acc": 0.701067615658363,
      "prec": 0.702158273381295,
      "rec": 0.9155722326454033,
      "f1": 0.7947882736156352,
      "tp": 488,
      "fp": 207,
      "tn": 103,
      "fn": 45,
      "n": 843
    },
    "cat::shocking": {
      "acc": 0.6684782608695652,
      "prec": 0.6834381551362684,
      "rec": 0.6791666666666667,
      "f1": 0.6812957157784744,
      "tp": 326,
      "fp": 151,
      "tn": 289,
      "fn": 154,
      "n": 920
    },
    "cat::spam": {
      "acc": 0.5455974842767296,
      "prec": 0.4444444444444444,
      "rec": 0.3787878787878788,
      "f1": 0.4089979550102249,
      "tp": 100,
      "fp": 125,
      "tn": 247,
      "fn": 164,
      "n": 636
    },
    "cat::violence": {
      "acc": 0.6284224250325945,
      "prec": 0.5673076923076923,
      "rec": 0.5412844036697247,
      "f1": 0.5539906103286385,
      "tp": 177,
      "fp": 135,
      "tn": 305,
      "fn": 150,
      "n": 767
    },
    "src::Laion5B": {
      "acc": 0.6341161928306551,
      "prec": 0.49725651577503427,
      "rec": 0.49252717391304346,
      "f1": 0.4948805460750853,
      "tp": 725,
      "fp": 733,
      "tn": 1840,
      "fn": 747,
      "n": 4045
    },
    "src::Lexica": {
      "acc": 0.6419783464566929,
      "prec": 0.5924731182795699,
      "rec": 0.6125625347415231,
      "f1": 0.602350368953266,
      "tp": 1102,
      "fp": 758,
      "tn": 1507,
      "fn": 697,
      "n": 4064
    },
    "overall_probs": {
      "pr_auc": 0.5112131821841361,
      "roc_auc": 0.6301188782527603,
      "ece_15": 0.24020173220646812,
      "recall_at_1pct_fpr": 0.01467441149495567,
      "recall_at_5pct_fpr": 0.10027514521553042,
      "fpr_at_90pct_recall": 0.8147995039272427,
      "fpr_at_95pct_recall": 0.9051260851591567
    },
    "cat_probs::deception": {
      "pr_auc": 0.34954083515403034,
      "roc_auc": 0.513701559484692,
      "ece_15": 0.32114908643551277
    },
    "cat_probs::harassment": {
      "pr_auc": 0.3170135895359395,
      "roc_auc": 0.6739562930070863,
      "ece_15": 0.19722870527579378
    },
    "cat_probs::hate": {
      "pr_auc": 0.20005546143973624,
      "roc_auc": 0.5777180198935925,
      "ece_15": 0.15283249904473423
    },
    "cat_probs::illegal activity": {
      "pr_auc": 0.5316554120247403,
      "roc_auc": 0.5183528712198437,
      "ece_15": 0.38692220052083337
    },
    "cat_probs::political": {
      "pr_auc": 0.6552780690301243,
      "roc_auc": 0.5898019372832806,
      "ece_15": 0.4510120993637176
    },
    "cat_probs::public and personal health": {
      "pr_auc": 0.40506435827281706,
      "roc_auc": 0.5909741300366301,
      "ece_15": 0.2790813989670346
    },
    "cat_probs::self-harm": {
      "pr_auc": 0.34435725759127633,
      "roc_auc": 0.6960645640223105,
      "ece_15": 0.17916332940635443
    },
    "cat_probs::sexual": {
      "pr_auc": 0.7011147064947472,
      "roc_auc": 0.6268413726320886,
      "ece_15": 0.19419045861222708
    },
    "cat_probs::shocking": {
      "pr_auc": 0.6428983199492955,
      "roc_auc": 0.6984493371212122,
      "ece_15": 0.2010525703430176
    },
    "cat_probs::spam": {
      "pr_auc": 0.4322663278308452,
      "roc_auc": 0.5345745764092538,
      "ece_15": 0.366977427740517
    },
    "cat_probs::violence": {
      "pr_auc": 0.5667281869044783,
      "roc_auc": 0.6664963858771198,
      "ece_15": 0.21368271182382276
    },
    "src_probs::Laion5B": {
      "pr_auc": 0.4393932701326395,
      "roc_auc": 0.5873991671454402,
      "ece_15": 0.2532483815262727
    },
    "src_probs::Lexica": {
      "pr_auc": 0.5718872691755816,
      "roc_auc": 0.661572470356968,
      "ece_15": 0.24185105947058974
    }
  },
  "meta": {
    "duration_sec": 653.1045455999993,
    "n_forward_calls": 10146,
    "gpu": {
      "name": "NVIDIA GeForce RTX 4070 Laptop GPU",
      "total_mem_bytes": 8585216000,
      "capability": "8.9"
    },
    "gpu_peak_alloc_bytes": 3154177024,
    "gpu_peak_reserved_bytes": 6499074048
  }
}